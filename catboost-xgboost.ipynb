{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DATA MINING PROJECT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aybike Battal - 150200343**\n",
    "\n",
    "\n",
    "**Ä°layda Kara - 150220747**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from sklearn.preprocessing import StandardScaler, MultiLabelBinarizer, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.utils import resample\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read files and convert them to DataFrames\n",
    "product_catalog = pd.read_csv(\"product_catalog.csv\")\n",
    "product_category_map = pd.read_csv(\"product_category_map.csv\")\n",
    "transactions = pd.read_csv(\"transactions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info for product_catalog df:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32776 entries, 0 to 32775\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   product_id       32776 non-null  int64 \n",
      " 1   manufacturer_id  32776 non-null  int64 \n",
      " 2   attribute_1      32776 non-null  int64 \n",
      " 3   attribute_2      32776 non-null  int64 \n",
      " 4   attribute_3      32776 non-null  int64 \n",
      " 5   attribute_4      32776 non-null  int64 \n",
      " 6   attribute_5      32776 non-null  int64 \n",
      " 7   categories       25988 non-null  object\n",
      "dtypes: int64(7), object(1)\n",
      "memory usage: 2.0+ MB\n",
      "None\n",
      "\n",
      "Number of Duplicate Rows:\n",
      "0\n",
      "\n",
      "Missing Value Number for Each Attribute:\n",
      "product_id            0\n",
      "manufacturer_id       0\n",
      "attribute_1           0\n",
      "attribute_2           0\n",
      "attribute_3           0\n",
      "attribute_4           0\n",
      "attribute_5           0\n",
      "categories         6788\n",
      "dtype: int64\n",
      "\n",
      "First three rows of df:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>manufacturer_id</th>\n",
       "      <th>attribute_1</th>\n",
       "      <th>attribute_2</th>\n",
       "      <th>attribute_3</th>\n",
       "      <th>attribute_4</th>\n",
       "      <th>attribute_5</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22665</td>\n",
       "      <td>861</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>490</td>\n",
       "      <td>2</td>\n",
       "      <td>66</td>\n",
       "      <td>[2890, 855, 3908, 3909]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28640</td>\n",
       "      <td>1366</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>537</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13526</td>\n",
       "      <td>1090</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>511</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[3270, 163, 284, 1694, 12, 3837, 2422, 3595, 3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id  manufacturer_id  attribute_1  attribute_2  attribute_3  \\\n",
       "0       22665              861            4            0          490   \n",
       "1       28640             1366           10            1          537   \n",
       "2       13526             1090           10            0          511   \n",
       "\n",
       "   attribute_4  attribute_5                                         categories  \n",
       "0            2           66                            [2890, 855, 3908, 3909]  \n",
       "1            0          101                                                NaN  \n",
       "2            0            0  [3270, 163, 284, 1694, 12, 3837, 2422, 3595, 3...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Information about Product Catalog Dataset\n",
    "print(\"Info for product_catalog df:\")\n",
    "print(product_catalog.info())\n",
    "print()\n",
    "print(\"Number of Duplicate Rows:\")\n",
    "print(product_catalog.duplicated().sum())\n",
    "print()\n",
    "print(\"Missing Value Number for Each Attribute:\")\n",
    "print(product_catalog.isnull().sum())\n",
    "print()\n",
    "print(\"First three rows of df:\")\n",
    "display(product_catalog.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info for product_category_map df:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4332 entries, 0 to 4331\n",
      "Data columns (total 2 columns):\n",
      " #   Column              Non-Null Count  Dtype\n",
      "---  ------              --------------  -----\n",
      " 0   category_id         4332 non-null   int64\n",
      " 1   parent_category_id  4332 non-null   int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 67.8 KB\n",
      "None\n",
      "\n",
      "Missing Value Number for Each Attribute:\n",
      "category_id           0\n",
      "parent_category_id    0\n",
      "dtype: int64\n",
      "\n",
      "Number of Duplicate Rows:\n",
      "0\n",
      "\n",
      "First three rows of df:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_id</th>\n",
       "      <th>parent_category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1082</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category_id  parent_category_id\n",
       "0            0                  75\n",
       "1            1                1499\n",
       "2            2                1082"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Information about Product Category Map Dataset\n",
    "print(\"Info for product_category_map df:\")\n",
    "print(product_category_map.info())\n",
    "print()\n",
    "print(\"Missing Value Number for Each Attribute:\")\n",
    "print(product_category_map.isnull().sum())\n",
    "print()\n",
    "print(\"Number of Duplicate Rows:\")\n",
    "print(product_category_map.duplicated().sum())\n",
    "print()\n",
    "print(\"First three rows of df:\")\n",
    "display(product_category_map.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info for transactions df:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1071538 entries, 0 to 1071537\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count    Dtype \n",
      "---  ------         --------------    ----- \n",
      " 0   customer_id    1071538 non-null  int64 \n",
      " 1   product_id     1071538 non-null  int64 \n",
      " 2   purchase_date  1071538 non-null  object\n",
      " 3   quantity       1071538 non-null  int64 \n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 32.7+ MB\n",
      "None\n",
      "\n",
      "Missing Value Number for Each Attribute:\n",
      "customer_id      0\n",
      "product_id       0\n",
      "purchase_date    0\n",
      "quantity         0\n",
      "dtype: int64\n",
      "\n",
      "First three rows of df:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>purchase_date</th>\n",
       "      <th>quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38769</td>\n",
       "      <td>3477</td>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42535</td>\n",
       "      <td>30474</td>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42535</td>\n",
       "      <td>15833</td>\n",
       "      <td>2020-06-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  product_id purchase_date  quantity\n",
       "0        38769        3477    2020-06-01         1\n",
       "1        42535       30474    2020-06-01         1\n",
       "2        42535       15833    2020-06-01         1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Information about Transactions Dataset\n",
    "print(\"Info for transactions df:\")\n",
    "print(transactions.info())\n",
    "print()\n",
    "print(\"Missing Value Number for Each Attribute:\")\n",
    "print(transactions.isnull().sum())\n",
    "print()\n",
    "print(\"First three rows of df:\")\n",
    "display(transactions.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_id         0\n",
      "manufacturer_id    0\n",
      "attribute_1        0\n",
      "attribute_2        0\n",
      "attribute_3        0\n",
      "attribute_4        0\n",
      "attribute_5        0\n",
      "categories         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Fill any missing values in the 'categories' column with an empty list '[]' \n",
    "# to ensure there are no null values in the column\n",
    "product_catalog['categories'] = product_catalog['categories'].fillna('[]')\n",
    "# Print the count of missing values in each column \n",
    "print(product_catalog.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string representation of lists in 'categories' column to actual Python lists\n",
    "product_catalog['categories'] = product_catalog['categories'].apply(literal_eval)\n",
    "\n",
    "# Encode categories using MultiLabelBinarizer to convert lists into binary format\n",
    "mlb = MultiLabelBinarizer()\n",
    "categories_encoded = mlb.fit_transform(product_catalog['categories'])\n",
    "\n",
    "# Normalize numerical features\n",
    "numerical_features = product_catalog[['attribute_1', 'attribute_2', 'attribute_3', 'attribute_4', 'attribute_5']]\n",
    "scaler = StandardScaler()\n",
    "numerical_features_normalized = scaler.fit_transform(numerical_features)\n",
    "\n",
    "# Combine the normalized numerical features and encoded categories into a single feature set\n",
    "features = np.hstack([numerical_features_normalized, categories_encoded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply KMeans clustering with 10 clusters and assign cluster labels to the data\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "product_catalog['cluster'] = kmeans.fit_predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'purchase_date' to datetime format\n",
    "transactions['purchase_date'] = pd.to_datetime(transactions['purchase_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new feature called 'shopping tendency'\n",
    "\n",
    "# Count the number of transactions per customer\n",
    "transaction_counts = transactions.groupby('customer_id')['purchase_date'].count().reset_index(name='transaction_count')\n",
    "\n",
    "# Discretize the transaction counts into categories from 1 to 10\n",
    "max_transactions = transaction_counts['transaction_count'].max()\n",
    "bins = np.linspace(0, max_transactions, 11)  # 10 bins\n",
    "labels = range(1, 11)  # Categories 1 to 10\n",
    "customer_categories = pd.cut(transaction_counts['transaction_count'], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# Add the discretized 'shopping_tendency' category to the transaction data\n",
    "transaction_counts['shopping_tendency'] = customer_categories\n",
    "\n",
    "# Merge transaction count and shopping tendency back into 'transactions' DataFrame\n",
    "transactions = pd.merge(transactions, transaction_counts[['customer_id', 'shopping_tendency']], on='customer_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Binning the transaction counts into categories: The number of transactions per customer is divided into 10 bins. Each bin represents a range of transaction counts, and each customer is placed into a category based on their transaction frequency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of days since the last purchase for each customer-product pair\n",
    "\n",
    "# Sort transactions by customer_id, product_id, and purchase_date\n",
    "transactions = transactions.sort_values(by=['customer_id', 'product_id', 'purchase_date'])\n",
    "\n",
    "# Calculate the previous purchase date for each customer-product pair\n",
    "transactions['prev_purchase_date'] = transactions.groupby(['customer_id', 'product_id'])['purchase_date'].shift(1)\n",
    "\n",
    "# Calculate the number of days since last purchase and fill missing values with 0\n",
    "transactions['days_since_last_purchase'] = (transactions['purchase_date'] - transactions['prev_purchase_date']).dt.days.fillna(0).astype(int)\n",
    "\n",
    "# Apply log transformation to 'days_since_last_purchase' for better scale\n",
    "transactions['log_days_since_last_purchase'] = np.log1p(transactions['days_since_last_purchase'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The 'days since last purchase' feature is calculated to assess the likelihood of a customer repurchasing the same product within a specified time frame.*\n",
    "\n",
    "*The 'log_days_since_last_purchase' feature is created by applying a log transformation to the 'days_since_last_purchase' in order to better handle large variations in the number of days. This transformation helps to reduce the impact of outliers and compresses the scale, making the data more suitable for modeling.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate product popularity based on the total number of transactions per product\n",
    "product_popularity = transactions.groupby('product_id')['customer_id'].count().reset_index(name='product_popularity')\n",
    "\n",
    "# Merge product popularity into 'transactions' DataFrame\n",
    "transactions = pd.merge(transactions, product_popularity[['product_id', 'product_popularity']], on='product_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The product_popularity feature is calculated to see how frequently a product is purchased across all customers, helping to understand the overall demand for each product.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the quantity of the last purchase for each customer-product pair\n",
    "transactions['last_quantity'] = transactions.groupby(['customer_id', 'product_id'])['quantity'].shift(1, fill_value=0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The 'last_quantity' feature is calculated to analyze if the quantity purchased in the previous transaction influences the time taken for a customer to repurchase a product. For example, purchasing a larger quantity might indicate a longer wait before the next purchase.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate days since the first purchase for each customer\n",
    "first_purchase = transactions.groupby('customer_id')['purchase_date'].min().reset_index()\n",
    "first_purchase.columns = ['customer_id', 'first_purchase_date']\n",
    "first_purchase['days_since_first_purchase'] = (pd.to_datetime('today') - first_purchase['first_purchase_date']).dt.days\n",
    "\n",
    "# Merge the 'days_since_first_purchase' feature into 'transactions' DataFrame\n",
    "transactions = pd.merge(transactions, first_purchase[['customer_id', 'days_since_first_purchase']], on='customer_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The 'days_since_first_purchase' feature is used to measure how long a customer has been using the platform. This can help in understanding customer behavior, as older customers may have different repurchase patterns than newer ones.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rolling average of quantity for each customer over the past 5 transactions\n",
    "transactions['rolling_avg_quantity'] = transactions.groupby('customer_id')['quantity'].rolling(5, min_periods=1).mean().reset_index(level=0, drop=True)\n",
    "\n",
    "# Calculate rolling mean of days between purchases for each customer\n",
    "transactions['rolling_avg_days_between'] = transactions.groupby('customer_id')['days_since_last_purchase'].rolling(5, min_periods=1).mean().reset_index(level=0, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The rolling average features are calculated to analyze the customer's past behavior and identify trends in their purchasing patterns. By considering the last 5 transactions, we can observe how the quantity purchased and the time between purchases impact future buying behavior. This approach helps in predicting when the customer is likely to repurchase, as past purchasing trends often influence future buying decisions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total quantity purchased by each customer\n",
    "total_quantity = transactions.groupby('customer_id')['quantity'].sum().reset_index(name='total_quantity')\n",
    "\n",
    "# Calculate the average quantity purchased per transaction for each customer\n",
    "average_quantity = transactions.groupby('customer_id')['quantity'].mean().reset_index(name='average_quantity')\n",
    "\n",
    "# Merge these features into a single DataFrame for customer_features\n",
    "customer_features = pd.merge(transaction_counts[['customer_id', 'transaction_count']], total_quantity, on='customer_id')\n",
    "customer_features = pd.merge(customer_features, average_quantity, on='customer_id')\n",
    "\n",
    "# Add preferred shopping time based on the mode of the day of week for each customer - Weekday or Weekend\n",
    "transactions['day_of_week'] = transactions['purchase_date'].dt.dayofweek\n",
    "transactions['is_weekend'] = transactions['day_of_week'].isin([5, 6])  # 5=Saturday, 6=Sunday\n",
    "customer_mode_day = transactions.groupby('customer_id')['day_of_week'].agg(lambda x: x.mode()[0]).reset_index()\n",
    "customer_mode_day.columns = ['customer_id', 'mode_day_of_week']\n",
    "transactions = pd.merge(transactions, customer_mode_day, on='customer_id', how='left')\n",
    "transactions['preferred_shopping_time'] = transactions['mode_day_of_week'].apply(lambda x: 'Weekend' if x in [5, 6] else 'Weekday')\n",
    "transactions['preferred_shopping_time_encoded'] = transactions['preferred_shopping_time'].apply(lambda x: 1 if x == 'Weekend' else 0)\n",
    "\n",
    "# Merge preferred shopping time encoded feature into the customer_features DataFrame\n",
    "customer_features = pd.merge(customer_features, transactions[['customer_id', 'preferred_shopping_time_encoded']].drop_duplicates(), on='customer_id', how='left')\n",
    "\n",
    "# Scale features to have a similar range and improve K-means performance\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(customer_features[['transaction_count', \n",
    "                                                         'total_quantity', \n",
    "                                                         'average_quantity', \n",
    "                                                         'preferred_shopping_time_encoded']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The features like total quantity, average quantity, and preferred shopping time are used to group customers with similar purchasing patterns for better analysis and predictions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply KMeans clustering with 10 clusters and assign cluster labels to the data\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "customer_features['cluster_customer'] = kmeans.fit_predict(scaled_features)\n",
    "\n",
    "# Merge customer clusters back into the transactions DataFrame\n",
    "transactions = pd.merge(transactions, customer_features[['customer_id', 'cluster_customer']], on='customer_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge transaction data with product catalog to add cluster information\n",
    "merged_df = pd.merge(\n",
    "    transactions,\n",
    "    product_catalog[['product_id', 'cluster']],  # Select columns for merging \n",
    "    on='product_id',\n",
    "    how='left'  \n",
    ")\n",
    "\n",
    "# Sort by customer_id, cluster, and purchase_date to keep the correct order for calculations.\n",
    "merged_df = merged_df.sort_values(by=['customer_id', 'cluster', 'purchase_date'])\n",
    "\n",
    "# Calculate cumulative sum of quantities for each customer-cluster to track total quantity over time\n",
    "merged_df['cumulative_quantity'] = merged_df.groupby(['customer_id', 'cluster'])['quantity'].cumsum()\n",
    "\n",
    "# Calculate the previous transaction's cumulative quantity\n",
    "merged_df['last_cumulative_quantity'] = merged_df.groupby(['customer_id', 'cluster'])['cumulative_quantity'].shift(1, fill_value=0).astype(int)\n",
    "\n",
    "# Calculate the last quantity purchased by the same customer for the same product\n",
    "merged_df['last_quantity'] = merged_df.groupby(['customer_id', 'product_id'])['quantity'].shift(1, fill_value=0).astype(int)\n",
    "\n",
    "# Select columns for training the model\n",
    "train_df = merged_df[['customer_id', 'product_id', 'cluster', 'last_quantity',\n",
    "                       'log_days_since_last_purchase', 'days_since_last_purchase', 'purchase_date', \n",
    "                       'quantity', 'cumulative_quantity', 'last_cumulative_quantity', \n",
    "                       'shopping_tendency', 'cluster_customer', 'product_popularity', \n",
    "                       'days_since_first_purchase', 'rolling_avg_quantity', 'rolling_avg_days_between']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Transactions and product catalog dataframes are merged to find features for training the model. Additional features, such as previous cumulative quantity are added to track how many products a customer has purchased from the same product cluster. These features help represent product preferences and purchasing patterns to support predictions of future purchases.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Training and Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Regression Approach using XGBoost and CatBoost**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train a model\n",
    "def train_model(X, y, model_type='catboost'):\n",
    "    if model_type == 'catboost':\n",
    "        model = CatBoostRegressor(iterations=100, learning_rate=0.1, depth=5, cat_features=['shopping_tendency'])\n",
    "    elif model_type == 'xgboost':\n",
    "        model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=5)\n",
    "    model.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aybike\\AppData\\Local\\Temp\\ipykernel_13052\\1551150429.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['shopping_tendency'] = le_shopping_tendency.fit_transform(X['shopping_tendency'])\n"
     ]
    }
   ],
   "source": [
    "X = train_df[['cluster', 'last_quantity','last_cumulative_quantity','shopping_tendency','cluster_customer',\n",
    "              'product_popularity','days_since_first_purchase','rolling_avg_quantity','rolling_avg_days_between']]  # Features\n",
    "y = train_df['log_days_since_last_purchase']  # Target\n",
    "\n",
    "# Encode categorical column \"shopping_tendency\"\n",
    "le_shopping_tendency = LabelEncoder()\n",
    "X['shopping_tendency'] = le_shopping_tendency.fit_transform(X['shopping_tendency'])\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.3876816\ttotal: 205ms\tremaining: 20.3s\n",
      "1:\tlearn: 1.2544545\ttotal: 289ms\tremaining: 14.2s\n",
      "2:\tlearn: 1.1349860\ttotal: 387ms\tremaining: 12.5s\n",
      "3:\tlearn: 1.0280431\ttotal: 474ms\tremaining: 11.4s\n",
      "4:\tlearn: 0.9325908\ttotal: 559ms\tremaining: 10.6s\n",
      "5:\tlearn: 0.8467461\ttotal: 660ms\tremaining: 10.3s\n",
      "6:\tlearn: 0.7701763\ttotal: 748ms\tremaining: 9.94s\n",
      "7:\tlearn: 0.7018195\ttotal: 835ms\tremaining: 9.6s\n",
      "8:\tlearn: 0.6411640\ttotal: 924ms\tremaining: 9.34s\n",
      "9:\tlearn: 0.5874708\ttotal: 1.02s\tremaining: 9.21s\n",
      "10:\tlearn: 0.5399205\ttotal: 1.12s\tremaining: 9.03s\n",
      "11:\tlearn: 0.4979230\ttotal: 1.21s\tremaining: 8.86s\n",
      "12:\tlearn: 0.4609181\ttotal: 1.31s\tremaining: 8.74s\n",
      "13:\tlearn: 0.4286787\ttotal: 1.39s\tremaining: 8.56s\n",
      "14:\tlearn: 0.4005836\ttotal: 1.46s\tremaining: 8.29s\n",
      "15:\tlearn: 0.3761641\ttotal: 1.53s\tremaining: 8.04s\n",
      "16:\tlearn: 0.3552426\ttotal: 1.6s\tremaining: 7.81s\n",
      "17:\tlearn: 0.3371469\ttotal: 1.67s\tremaining: 7.6s\n",
      "18:\tlearn: 0.3216545\ttotal: 1.74s\tremaining: 7.42s\n",
      "19:\tlearn: 0.3083992\ttotal: 1.81s\tremaining: 7.25s\n",
      "20:\tlearn: 0.2972345\ttotal: 1.88s\tremaining: 7.09s\n",
      "21:\tlearn: 0.2877788\ttotal: 1.95s\tremaining: 6.92s\n",
      "22:\tlearn: 0.2798910\ttotal: 2.02s\tremaining: 6.76s\n",
      "23:\tlearn: 0.2733237\ttotal: 2.08s\tremaining: 6.6s\n",
      "24:\tlearn: 0.2677950\ttotal: 2.15s\tremaining: 6.45s\n",
      "25:\tlearn: 0.2631921\ttotal: 2.22s\tremaining: 6.32s\n",
      "26:\tlearn: 0.2593887\ttotal: 2.29s\tremaining: 6.19s\n",
      "27:\tlearn: 0.2562247\ttotal: 2.35s\tremaining: 6.05s\n",
      "28:\tlearn: 0.2536135\ttotal: 2.42s\tremaining: 5.93s\n",
      "29:\tlearn: 0.2513175\ttotal: 2.49s\tremaining: 5.81s\n",
      "30:\tlearn: 0.2494404\ttotal: 2.55s\tremaining: 5.69s\n",
      "31:\tlearn: 0.2479453\ttotal: 2.62s\tremaining: 5.56s\n",
      "32:\tlearn: 0.2466663\ttotal: 2.69s\tremaining: 5.45s\n",
      "33:\tlearn: 0.2456203\ttotal: 2.75s\tremaining: 5.34s\n",
      "34:\tlearn: 0.2447495\ttotal: 2.82s\tremaining: 5.24s\n",
      "35:\tlearn: 0.2440469\ttotal: 2.88s\tremaining: 5.13s\n",
      "36:\tlearn: 0.2433991\ttotal: 2.95s\tremaining: 5.03s\n",
      "37:\tlearn: 0.2428925\ttotal: 3.02s\tremaining: 4.93s\n",
      "38:\tlearn: 0.2424462\ttotal: 3.09s\tremaining: 4.83s\n",
      "39:\tlearn: 0.2420704\ttotal: 3.16s\tremaining: 4.75s\n",
      "40:\tlearn: 0.2417446\ttotal: 3.23s\tremaining: 4.65s\n",
      "41:\tlearn: 0.2414428\ttotal: 3.3s\tremaining: 4.56s\n",
      "42:\tlearn: 0.2412235\ttotal: 3.37s\tremaining: 4.46s\n",
      "43:\tlearn: 0.2410035\ttotal: 3.44s\tremaining: 4.37s\n",
      "44:\tlearn: 0.2408041\ttotal: 3.5s\tremaining: 4.28s\n",
      "45:\tlearn: 0.2406407\ttotal: 3.58s\tremaining: 4.2s\n",
      "46:\tlearn: 0.2403492\ttotal: 3.67s\tremaining: 4.14s\n",
      "47:\tlearn: 0.2400914\ttotal: 3.74s\tremaining: 4.05s\n",
      "48:\tlearn: 0.2398720\ttotal: 3.83s\tremaining: 3.98s\n",
      "49:\tlearn: 0.2397067\ttotal: 3.89s\tremaining: 3.89s\n",
      "50:\tlearn: 0.2395614\ttotal: 3.96s\tremaining: 3.8s\n",
      "51:\tlearn: 0.2394435\ttotal: 4.03s\tremaining: 3.72s\n",
      "52:\tlearn: 0.2393489\ttotal: 4.09s\tremaining: 3.63s\n",
      "53:\tlearn: 0.2392278\ttotal: 4.16s\tremaining: 3.54s\n",
      "54:\tlearn: 0.2391521\ttotal: 4.22s\tremaining: 3.46s\n",
      "55:\tlearn: 0.2390651\ttotal: 4.28s\tremaining: 3.36s\n",
      "56:\tlearn: 0.2389597\ttotal: 4.34s\tremaining: 3.28s\n",
      "57:\tlearn: 0.2388693\ttotal: 4.41s\tremaining: 3.19s\n",
      "58:\tlearn: 0.2387788\ttotal: 4.48s\tremaining: 3.11s\n",
      "59:\tlearn: 0.2387082\ttotal: 4.54s\tremaining: 3.03s\n",
      "60:\tlearn: 0.2386599\ttotal: 4.61s\tremaining: 2.94s\n",
      "61:\tlearn: 0.2386184\ttotal: 4.67s\tremaining: 2.86s\n",
      "62:\tlearn: 0.2385124\ttotal: 4.73s\tremaining: 2.78s\n",
      "63:\tlearn: 0.2384368\ttotal: 4.8s\tremaining: 2.7s\n",
      "64:\tlearn: 0.2384017\ttotal: 4.87s\tremaining: 2.62s\n",
      "65:\tlearn: 0.2383580\ttotal: 4.93s\tremaining: 2.54s\n",
      "66:\tlearn: 0.2383113\ttotal: 5s\tremaining: 2.46s\n",
      "67:\tlearn: 0.2382762\ttotal: 5.06s\tremaining: 2.38s\n",
      "68:\tlearn: 0.2382436\ttotal: 5.15s\tremaining: 2.31s\n",
      "69:\tlearn: 0.2381883\ttotal: 5.24s\tremaining: 2.25s\n",
      "70:\tlearn: 0.2381307\ttotal: 5.34s\tremaining: 2.18s\n",
      "71:\tlearn: 0.2380746\ttotal: 5.42s\tremaining: 2.11s\n",
      "72:\tlearn: 0.2380167\ttotal: 5.51s\tremaining: 2.04s\n",
      "73:\tlearn: 0.2379551\ttotal: 5.59s\tremaining: 1.97s\n",
      "74:\tlearn: 0.2378996\ttotal: 5.68s\tremaining: 1.89s\n",
      "75:\tlearn: 0.2378606\ttotal: 5.77s\tremaining: 1.82s\n",
      "76:\tlearn: 0.2378182\ttotal: 5.83s\tremaining: 1.74s\n",
      "77:\tlearn: 0.2377591\ttotal: 5.89s\tremaining: 1.66s\n",
      "78:\tlearn: 0.2377130\ttotal: 5.96s\tremaining: 1.58s\n",
      "79:\tlearn: 0.2376752\ttotal: 6.03s\tremaining: 1.51s\n",
      "80:\tlearn: 0.2376569\ttotal: 6.11s\tremaining: 1.43s\n",
      "81:\tlearn: 0.2376164\ttotal: 6.18s\tremaining: 1.36s\n",
      "82:\tlearn: 0.2375886\ttotal: 6.25s\tremaining: 1.28s\n",
      "83:\tlearn: 0.2375550\ttotal: 6.32s\tremaining: 1.2s\n",
      "84:\tlearn: 0.2375074\ttotal: 6.39s\tremaining: 1.13s\n",
      "85:\tlearn: 0.2374583\ttotal: 6.45s\tremaining: 1.05s\n",
      "86:\tlearn: 0.2374343\ttotal: 6.52s\tremaining: 974ms\n",
      "87:\tlearn: 0.2374196\ttotal: 6.58s\tremaining: 898ms\n",
      "88:\tlearn: 0.2373763\ttotal: 6.65s\tremaining: 822ms\n",
      "89:\tlearn: 0.2373616\ttotal: 6.71s\tremaining: 746ms\n",
      "90:\tlearn: 0.2373446\ttotal: 6.77s\tremaining: 670ms\n",
      "91:\tlearn: 0.2373137\ttotal: 6.84s\tremaining: 595ms\n",
      "92:\tlearn: 0.2372823\ttotal: 6.9s\tremaining: 519ms\n",
      "93:\tlearn: 0.2372411\ttotal: 6.97s\tremaining: 445ms\n",
      "94:\tlearn: 0.2372275\ttotal: 7.03s\tremaining: 370ms\n",
      "95:\tlearn: 0.2372009\ttotal: 7.09s\tremaining: 296ms\n",
      "96:\tlearn: 0.2371727\ttotal: 7.16s\tremaining: 221ms\n",
      "97:\tlearn: 0.2371101\ttotal: 7.23s\tremaining: 148ms\n",
      "98:\tlearn: 0.2370689\ttotal: 7.3s\tremaining: 73.7ms\n",
      "99:\tlearn: 0.2370328\ttotal: 7.37s\tremaining: 0us\n"
     ]
    }
   ],
   "source": [
    "# Train CatBoost model\n",
    "catboost_model = train_model(X_train, y_train, model_type='catboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost model\n",
    "xgboost_model = train_model(X_train, y_train, model_type='xgboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for CatBoost Model: 15.650558096011206\n",
      "RMSE for XGBoost Model: 15.508764583193665\n"
     ]
    }
   ],
   "source": [
    "# Get predictions for the validation set in log scale\n",
    "catboost_predictions_log = catboost_model.predict(X_val)\n",
    "xgboost_predictions_log = xgboost_model.predict(X_val)\n",
    "\n",
    "# Convert predictions and actual values back to original scale\n",
    "catboost_predictions_original = np.exp(catboost_predictions_log)\n",
    "xgboost_predictions_original = np.exp(xgboost_predictions_log)\n",
    "y_val_original = np.exp(y_val)\n",
    "\n",
    "# Calculate the mean squared error in the original scale\n",
    "catboost_mse_original = mean_squared_error(y_val_original, catboost_predictions_original)\n",
    "xgboost_mse_original = mean_squared_error(y_val_original, xgboost_predictions_original)\n",
    "\n",
    "# Calculate the root mean squared error in the original scale\n",
    "catboost_rmse_original = np.sqrt(catboost_mse_original)\n",
    "xgboost_rmse_original = np.sqrt(xgboost_mse_original)\n",
    "\n",
    "# Show the root mean squared error for each model\n",
    "print(\"RMSE for CatBoost Model:\", catboost_rmse_original)\n",
    "print(\"RMSE for XGBoost Model:\", xgboost_rmse_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEST SET EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions and process results\n",
    "def make_predictions(model, test, X_test, train_df_last, feb_1st_2021, map_weeks_to_feb,model_type):\n",
    "    y_pred_log = model.predict(X_test)\n",
    "    \n",
    "    # Back-transform predictions\n",
    "    test['prediction'] = np.expm1(y_pred_log)\n",
    "\n",
    "    # Ensure the predicted days are integers\n",
    "    test['prediction'] = test['prediction'].clip(lower=0).astype(int)\n",
    "\n",
    "    # Merge the test data with the last purchase data to add the purchase date\n",
    "    test_df_merged = test.merge(train_df_last[['customer_id', 'product_id', 'purchase_date']], \n",
    "                                  on=['customer_id', 'product_id'], how='left')\n",
    "\n",
    "    # Initial predicted purchase date\n",
    "    test_df_merged['predicted_purchase_date'] = test_df_merged['purchase_date'] + pd.to_timedelta(test_df_merged['prediction'], unit='D')\n",
    "\n",
    "    # If the predicted date is before February 1, 2021, adjust the date\n",
    "    def adjust_date(row):\n",
    "        if(row['prediction']!=0):\n",
    "            while row['predicted_purchase_date'] < pd.to_datetime('2021-02-01'):\n",
    "                row['predicted_purchase_date'] += pd.to_timedelta(row['prediction'], unit='D')\n",
    "            return row['predicted_purchase_date']\n",
    "\n",
    "    test_df_merged['predicted_purchase_date'] = test_df_merged.apply(adjust_date, axis=1)\n",
    "\n",
    "    # Calculate the number of days from February 1, 2021\n",
    "    test_df_merged['days_from_feb_1st'] = (test_df_merged['predicted_purchase_date'] - feb_1st_2021).dt.days \n",
    "\n",
    "    # Apply the mapping function\n",
    "    test_df_merged['prediction'] = test_df_merged['days_from_feb_1st'].apply(map_weeks_to_feb).astype(int)\n",
    "\n",
    "    # Select relevant columns and save the result to a CSV file\n",
    "    df_selected = test_df_merged[['id', 'customer_id', 'product_id', 'prediction']]\n",
    "    print(f\"Predictions of model{model_type}:\")\n",
    "    print(df_selected['prediction'].value_counts())\n",
    "    df_selected.to_csv(f'predictions_{model_type}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map day values to target values - weeks\n",
    "def map_weeks_to_feb(days):\n",
    "    if days > 29:\n",
    "        return 0\n",
    "    elif 0 <= days <= 8:\n",
    "        return 1\n",
    "    elif 8 < days <= 15:\n",
    "        return 2\n",
    "    elif 15 < days <= 22:\n",
    "        return 3\n",
    "    elif 22 < days <= 29:\n",
    "        return 4\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aybike\\AppData\\Local\\Temp\\ipykernel_13052\\2919524360.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['purchase_date'] = pd.to_datetime(train_df['purchase_date'])\n"
     ]
    }
   ],
   "source": [
    "# Sort train_df by purchase_date in descending order for prediction\n",
    "train_df['purchase_date'] = pd.to_datetime(train_df['purchase_date'])\n",
    "train_df = train_df.sort_values(by='purchase_date', ascending=False)\n",
    "\n",
    "# Create a new DataFrame containing only the last purchase for each customer-product pair\n",
    "train_df_last = train_df.groupby(['customer_id', 'product_id']).first().reset_index()\n",
    "\n",
    "# Read test csv\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Merge cluster and quantity information from the training DataFrame into the test DataFrame\n",
    "test = pd.merge(test, train_df_last[['customer_id', 'product_id', 'cluster', 'quantity',\n",
    "                                     'cumulative_quantity','shopping_tendency','cluster_customer',\n",
    "                                     'product_popularity','days_since_first_purchase','rolling_avg_quantity',\n",
    "                                     'rolling_avg_days_between']], \n",
    "                   on=['customer_id', 'product_id'], how='left')\n",
    "\n",
    "# Rename the 'quantity' column to 'last_quantity'\n",
    "test = test.rename(columns={'quantity': 'last_quantity','cumulative_quantity': 'last_cumulative_quantity'})\n",
    "\n",
    "# Select relevant columns for testing\n",
    "X_test = test[['cluster', 'last_quantity','last_cumulative_quantity','shopping_tendency',\n",
    "               'cluster_customer','product_popularity','days_since_first_purchase','rolling_avg_quantity',\n",
    "               'rolling_avg_days_between']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions of modelcatboost_model:\n",
      "prediction\n",
      "0    5652\n",
      "1    1193\n",
      "4    1085\n",
      "3    1039\n",
      "2    1031\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "feb_1st_2021 = pd.to_datetime('2021-02-01')\n",
    "\n",
    "# Test CatBoost model\n",
    "make_predictions(catboost_model, test, X_test, train_df_last, feb_1st_2021, map_weeks_to_feb,\"catboost_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions of modelxgboost_model:\n",
      "prediction\n",
      "0    5824\n",
      "1    1131\n",
      "4    1051\n",
      "3    1042\n",
      "2     952\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Test XGBoost model\n",
    "make_predictions(xgboost_model, test, X_test, train_df_last, feb_1st_2021, map_weeks_to_feb,\"xgboost_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Classification Approach using XGBoost**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undersample Majority Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample_and_train(X, y):\n",
    "    \n",
    "    df_train = pd.concat([X, y], axis=1)\n",
    "\n",
    "    df_majority = df_train[df_train['target_weeks'] == 0]  # Majority class (0 weeks)\n",
    "    df_minority = df_train[df_train['target_weeks'] != 0] \n",
    "\n",
    "    # Undersample the majority class to match the size of the minority class\n",
    "    df_majority_undersampled = resample(df_majority, \n",
    "                                        replace=False,      # No replacement\n",
    "                                        n_samples=len(df_minority),  # Match the minority class size\n",
    "                                        random_state=42)\n",
    "\n",
    "    # Combine the undersampled majority class with the minority class\n",
    "    df_train_balanced = pd.concat([df_majority_undersampled, df_minority])\n",
    "\n",
    "    # Shuffle the combined dataframe\n",
    "    df_train_balanced = df_train_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Separate the features and target again after balancing\n",
    "    X_train_balanced = df_train_balanced.drop('target_weeks', axis=1)\n",
    "    y_train_balanced = df_train_balanced['target_weeks']\n",
    "\n",
    "    # Train an XGBoost model on the balanced training data\n",
    "    model = XGBClassifier(objective='multi:softmax', num_class=5, n_estimators=100, learning_rate=0.1, max_depth=5)\n",
    "    model.fit(X_train_balanced, y_train_balanced)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversample Minority Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample_and_train(X, y):\n",
    "    df_train = pd.concat([X, y], axis=1)\n",
    "\n",
    "    df_majority = df_train[df_train['target_weeks'] == 0]  # Majority class (0 weeks)\n",
    "    df_minority = df_train[df_train['target_weeks'] != 0]  # Minority classes\n",
    "\n",
    "    # Oversample the minority class to match the size of the majority class\n",
    "    df_minority_oversampled = resample(df_minority, \n",
    "                                       replace=True,      # With replacement\n",
    "                                       n_samples=len(df_majority),  # Match the majority class size\n",
    "                                       random_state=42)\n",
    "\n",
    "    # Combine the oversampled minority class with the majority class\n",
    "    df_train_balanced = pd.concat([df_majority, df_minority_oversampled])\n",
    "\n",
    "    # Shuffle the combined dataframe\n",
    "    df_train_balanced = df_train_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Separate the features and target again after balancing\n",
    "    X_train_balanced = df_train_balanced.drop('target_weeks', axis=1)\n",
    "    y_train_balanced = df_train_balanced['target_weeks']\n",
    "\n",
    "    # Train an XGBoost model on the balanced training data\n",
    "    model = XGBClassifier(objective='multi:softmax', num_class=5, n_estimators=100, learning_rate=0.1, max_depth=5)\n",
    "    model.fit(X_train_balanced, y_train_balanced)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same Sample Size For All Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_class_size_and_train(X, y):\n",
    "    \n",
    "    # Combine features and target into one DataFrame\n",
    "    train_data = pd.concat([X, y], axis=1)\n",
    "\n",
    "    min_sample_size = train_data['target_weeks'].value_counts().min()\n",
    "\n",
    "    # Resample each class\n",
    "    balanced_train_data = pd.concat([\n",
    "        resample(train_data[train_data['target_weeks'] == cls],\n",
    "                replace=False,  # No replacement\n",
    "                n_samples=min_sample_size,  \n",
    "                random_state=42)\n",
    "        for cls in train_data['target_weeks'].unique()\n",
    "    ])\n",
    "    # Shuffle the balanced data\n",
    "    balanced_train_data = balanced_train_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Separate features and target\n",
    "    X_balanced = balanced_train_data.drop(columns=['target_weeks'])\n",
    "    y_balanced = balanced_train_data['target_weeks']\n",
    "\n",
    "    # Train an XGBoost Classifier\n",
    "    model = XGBClassifier(objective='multi:softmax', num_class=5, n_estimators=100, learning_rate=0.1, max_depth=5)\n",
    "\n",
    "    # Fit the model on the entire balanced dataset\n",
    "    model.fit(X_balanced, y_balanced)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target_weeks\n",
       "0    907570\n",
       "4     10820\n",
       "3      8122\n",
       "2      5374\n",
       "1      3641\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the 'days_since_last_purchase' into weeks and create target_weeks column\n",
    "train_df['target_weeks'] = (train_df['days_since_last_purchase'] / 7).astype(int)\n",
    "\n",
    "# Filter rows where the target_weeks is between 0 and 4 \n",
    "train_df = train_df[(train_df['target_weeks'] >= 0) & (train_df['target_weeks'] <= 4)]\n",
    "\n",
    "def map_weeks(days):\n",
    "    if 0 < days <= 7:\n",
    "        return 1\n",
    "    elif 7 < days <= 14:\n",
    "        return 2\n",
    "    elif 14 < days <= 21:\n",
    "        return 3\n",
    "    elif 21 < days <= 28:\n",
    "        return 4\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply the mapping function to 'days_since_last_purchase' to create 'target_weeks'\n",
    "train_df['target_weeks'] = train_df['days_since_last_purchase'].apply(map_weeks)\n",
    "train_df['target_weeks'] .value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the 'shopping_tendency' column as numbers\n",
    "le_shopping_tendency = LabelEncoder()\n",
    "train_df_encoded = train_df.copy()\n",
    "train_df_encoded['shopping_tendency'] = le_shopping_tendency.fit_transform(train_df_encoded['shopping_tendency'])\n",
    "\n",
    "X = train_df_encoded[['cluster', 'last_quantity','last_cumulative_quantity','shopping_tendency','cluster_customer',\n",
    "              'product_popularity','days_since_first_purchase','rolling_avg_quantity','rolling_avg_days_between']]  # Features\n",
    "y = train_df_encoded['target_weeks']  # Target\n",
    "\n",
    "# Split data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model and print classification report\n",
    "def evaluate_model(model, X_val, y_val):\n",
    "    # Predict the validation set\n",
    "    y_pred = model.predict(X_val)\n",
    "    # Print classification report\n",
    "    print(classification_report(y_val, y_pred, digits=2))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of XGBoost Classifier Trained on Split Training Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99    181515\n",
      "           1       0.90      0.42      0.57       728\n",
      "           2       0.64      0.31      0.42      1075\n",
      "           3       0.55      0.28      0.37      1624\n",
      "           4       0.43      0.52      0.47      2164\n",
      "\n",
      "    accuracy                           0.98    187106\n",
      "   macro avg       0.70      0.50      0.56    187106\n",
      "weighted avg       0.98      0.98      0.98    187106\n",
      "\n",
      "Evaluation of XGBoost Classifier Trained on Undersampled Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99    181515\n",
      "           1       0.95      0.47      0.63       728\n",
      "           2       0.68      0.31      0.43      1075\n",
      "           3       0.42      0.32      0.36      1624\n",
      "           4       0.35      0.92      0.51      2164\n",
      "\n",
      "    accuracy                           0.98    187106\n",
      "   macro avg       0.68      0.60      0.59    187106\n",
      "weighted avg       0.99      0.98      0.98    187106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the XGBoost classifier on the split training data\n",
    "xgboost_class = XGBClassifier(objective='multi:softmax', num_class=5, n_estimators=100, learning_rate=0.1, max_depth=5)\n",
    "xgboost_class.fit(X_train, y_train)\n",
    "\n",
    "print(\"Evaluation of XGBoost Classifier Trained on Split Training Data:\")\n",
    "evaluate_model(xgboost_class, X_val, y_val)\n",
    "\n",
    "# Train the XGBoost classifier on the undersampled data\n",
    "xgboost_class_undersample = undersample_and_train(X_train, y_train)\n",
    "\n",
    "print(\"Evaluation of XGBoost Classifier Trained on Undersampled Data:\")\n",
    "evaluate_model(xgboost_class_undersample, X_val, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of XGBoost Classifier Trained on Oversampled Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99    181515\n",
      "           1       0.95      0.47      0.63       728\n",
      "           2       0.69      0.31      0.43      1075\n",
      "           3       0.42      0.32      0.37      1624\n",
      "           4       0.35      0.92      0.51      2164\n",
      "\n",
      "    accuracy                           0.98    187106\n",
      "   macro avg       0.68      0.60      0.58    187106\n",
      "weighted avg       0.99      0.98      0.98    187106\n",
      "\n",
      "Evaluation of XGBoost Classifier Trained on Balanced Data:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99    181515\n",
      "           1       0.63      0.50      0.55       728\n",
      "           2       0.42      0.39      0.41      1075\n",
      "           3       0.34      0.40      0.37      1624\n",
      "           4       0.37      0.75      0.50      2164\n",
      "\n",
      "    accuracy                           0.97    187106\n",
      "   macro avg       0.55      0.61      0.56    187106\n",
      "weighted avg       0.98      0.97      0.98    187106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the XGBoost classifier on the oversampled data\n",
    "xgboost_class_oversample = oversample_and_train(X_train, y_train)\n",
    "\n",
    "print(\"Evaluation of XGBoost Classifier Trained on Oversampled Data:\")\n",
    "evaluate_model(xgboost_class_oversample, X_val, y_val)\n",
    "\n",
    "# Train the XGBoost classifier on the data that have same number of samples from each class\n",
    "xgboost_same_size_class = same_class_size_and_train(X_train, y_train)\n",
    "\n",
    "print(\"Evaluation of XGBoost Classifier Trained on Balanced Data:\")\n",
    "evaluate_model(xgboost_same_size_class, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of Classification Models on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort train_df by purchase_date in descending order for prediction\n",
    "train_df['purchase_date'] = pd.to_datetime(train_df['purchase_date'])\n",
    "train_df = train_df.sort_values(by='purchase_date', ascending=False)\n",
    "\n",
    "# Create a new DataFrame containing only the last purchase for each customer-product pair\n",
    "train_df_last = train_df.groupby(['customer_id', 'product_id']).first().reset_index()\n",
    "\n",
    "# Read test csv\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Merge cluster and quantity information from the training DataFrame into the test DataFrame\n",
    "test = pd.merge(test, train_df_last[['customer_id', 'product_id', 'cluster', 'quantity',\n",
    "                                     'cumulative_quantity','shopping_tendency','cluster_customer',\n",
    "                                     'product_popularity','days_since_first_purchase','rolling_avg_quantity',\n",
    "                                     'rolling_avg_days_between']], \n",
    "                   on=['customer_id', 'product_id'], how='left')\n",
    "\n",
    "# Rename the 'quantity' column to 'last_quantity'\n",
    "test = test.rename(columns={'quantity': 'last_quantity','cumulative_quantity': 'last_cumulative_quantity'})\n",
    "\n",
    "# Select relevant columns for testing\n",
    "X_test = test[['cluster', 'last_quantity','last_cumulative_quantity','shopping_tendency',\n",
    "               'cluster_customer','product_popularity','days_since_first_purchase','rolling_avg_quantity',\n",
    "               'rolling_avg_days_between']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate the model on test data and write predictions to a CSV file\n",
    "def evaluate_and_write_to_csv(test, X_test, model, name):\n",
    "    test['prediction'] = model.predict(X_test)  # Predict test set\n",
    "    print(test['prediction'].value_counts())\n",
    "    df_selected = test[['id', 'customer_id', 'product_id', 'prediction']]\n",
    "    df_selected.to_csv(f'predictions_{name}.csv', index=False)  # Save as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction\n",
      "0    5849\n",
      "4    1987\n",
      "1    1602\n",
      "3     374\n",
      "2     188\n",
      "Name: count, dtype: int64\n",
      "prediction\n",
      "4    5477\n",
      "0    3653\n",
      "3     597\n",
      "2     179\n",
      "1      94\n",
      "Name: count, dtype: int64\n",
      "prediction\n",
      "4    5471\n",
      "0    3675\n",
      "3     609\n",
      "2     167\n",
      "1      78\n",
      "Name: count, dtype: int64\n",
      "prediction\n",
      "4    4228\n",
      "1    3905\n",
      "3    1371\n",
      "2     496\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "evaluate_and_write_to_csv(test, X_test, xgboost_class, \"xgboost_class\")\n",
    "evaluate_and_write_to_csv(test, X_test, xgboost_class_undersample, \"xgboost_class_undersample\")\n",
    "evaluate_and_write_to_csv(test, X_test, xgboost_class_oversample, \"xgboost_class_oversample\")\n",
    "evaluate_and_write_to_csv(test, X_test, xgboost_same_size_class, \"xgboost_sames_size_class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-Stage Prediction Model\n",
    "\n",
    "1: Predict if customer will buy in 4 weeks or not\n",
    "\n",
    "2: Predict in which week will buy the product if s/he will buy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation of 2-staged model on validation set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99    181556\n",
      "           1       0.89      0.43      0.58       694\n",
      "           2       0.72      0.36      0.48      1074\n",
      "           3       0.59      0.34      0.44      1584\n",
      "           4       0.37      0.92      0.53      2198\n",
      "\n",
      "    accuracy                           0.98    187106\n",
      "   macro avg       0.72      0.61      0.61    187106\n",
      "weighted avg       0.99      0.98      0.98    187106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert y to binary for the first model (0 for no purchase, 1 for purchase)\n",
    "y_train_binary = (y_train != 0).astype(int)  # Class 0 stays 0, all others become 1\n",
    "y_val_binary = (y_val != 0).astype(int)\n",
    "\n",
    "# Train the binary classifier to predict if a customer will buy or not\n",
    "binary_model = XGBClassifier(objective='binary:logistic', n_estimators=100, learning_rate=0.1, max_depth=5)\n",
    "binary_model.fit(X_train, y_train_binary)\n",
    "\n",
    "# Make predictions with the binary model on the validation set\n",
    "binary_predictions_val = binary_model.predict(X_val)\n",
    "\n",
    "# Identify validation samples predicted to buy \n",
    "val_indices_non_0 = binary_predictions_val == 1  # Customers predicted to buy\n",
    "X_val_non_0 = X_val[val_indices_non_0]\n",
    "y_val_non_0 = y_val[val_indices_non_0] - 1  # Map classes to [0, 1, 2, 3]\n",
    "\n",
    "# Train the multi-class model to predict which week the customer will buy\n",
    "train_indices_non_0 = y_train_binary == 1  \n",
    "X_train_non_0 = X_train[train_indices_non_0]\n",
    "y_train_non_0 = y_train[train_indices_non_0] - 1  # Map classes to [0, 1, 2, 3]\n",
    "\n",
    "multi_class_model = XGBClassifier(objective='multi:softmax', num_class=4, n_estimators=100, learning_rate=0.1, max_depth=5)\n",
    "multi_class_model.fit(X_train_non_0, y_train_non_0)\n",
    "\n",
    "# Make predictions with the multi-class model for customers predicted to buy\n",
    "multi_class_predictions_val = multi_class_model.predict(X_val_non_0)\n",
    "\n",
    "# Map multi-class predictions back to original weeks (1, 2, 3, 4)\n",
    "multi_class_predictions_val = multi_class_predictions_val + 1\n",
    "\n",
    "# Create a combined prediction array for the validation set\n",
    "combined_predictions_val = binary_predictions_val.copy()\n",
    "combined_predictions_val[val_indices_non_0] = multi_class_predictions_val\n",
    "\n",
    "# Print the classification report for the validation set\n",
    "print(\"Evaluation of 2-stage model on validation set:\")\n",
    "print(classification_report(y_val, combined_predictions_val))\n",
    "\n",
    "# Make predictions with the binary model on the test data\n",
    "binary_predictions_test = binary_model.predict(X_test)\n",
    "\n",
    "# Identify non-0 predictions and predict which week they will buy using the multi-class model\n",
    "test_indices_non_0 = binary_predictions_test == 1  # Samples predicted as 1 by the binary model\n",
    "X_test_non_0 = X_test[test_indices_non_0]\n",
    "\n",
    "# Predict the week of purchase for customers who are predicted to buy\n",
    "multi_class_predictions_test = multi_class_model.predict(X_test_non_0)\n",
    "\n",
    "# Map predictions back to the original weeks (1, 2, 3, 4)\n",
    "multi_class_predictions_test = multi_class_predictions_test + 1\n",
    "\n",
    "# Add the binary predictions and multi-class predictions to the test data\n",
    "test['prediction'] = binary_predictions_test\n",
    "test.loc[test_indices_non_0, 'prediction'] = multi_class_predictions_test  # Replace with multi-class predictions for those who will buy\n",
    "\n",
    "# Save the final predictions to a CSV file\n",
    "test[['id', 'customer_id', 'product_id', 'prediction']].to_csv('2-stage-model-predictions.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prediction\n",
       "4    5362\n",
       "0    2267\n",
       "1    1725\n",
       "3     480\n",
       "2     166\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['prediction'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
